%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   Numerical optimization
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   The gradient method with constant step length ?
%   and step length ? computed by backtracking procedure
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
clear;
clc; 

x0 = [5;5];
tol = 1e-5;
kmax = 1e5;
[xk, k, fxk, xk_values, vecf, vecg, exit_cond] = gradiend_method(x0,@rosenbr, tol, kmax);

%[xk, k, fxk, xk_values, vecf, vecg, exit_cond] = newton_method(x0,@rosenbr, tol, kmax);


% plotting graphs with the norm of error as function of the iteration k
figure(1);
k_values = linspace(1,k+1, k+1);
xk_vector = ones(k+1,2);
for i = 1 : k + 1
    xk_vector(i) = 
    
end
norm_err = norm(xk_values(1:k+1,:) - xk_vector);
plot(k_values, norm_err);


% the function values as function of the iteration k


% gradient norms as function of the iteration k









